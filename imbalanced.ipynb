{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 17:50:51.966522: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-03 17:50:51.992076: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-08-03 17:50:51.992109: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-08-03 17:50:51.992126: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-08-03 17:50:51.997135: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ujjwal/miniconda3/envs/tf-nightly/lib/python3.9/site-packages/tensorflow/python/ops/distributions/distribution.py:259: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /home/ujjwal/miniconda3/envs/tf-nightly/lib/python3.9/site-packages/tensorflow/python/ops/distributions/bernoulli.py:165: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 17:50:52.462145: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28386 images belonging to 8 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 17:50:53.899408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1885] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1200 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__ShuffleDatasetV3_device_/job:localhost/replica:0/task:0/device:CPU:0}} buffer_size must be greater than zero or UNKNOWN_CARDINALITY [Op:ShuffleDatasetV3] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 92\u001b[0m\n\u001b[1;32m     89\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[1;32m     91\u001b[0m train_dataset \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices((augmented_data, train_labels))\n\u001b[0;32m---> 92\u001b[0m train_dataset \u001b[39m=\u001b[39m train_dataset\u001b[39m.\u001b[39;49mshuffle(buffer_size\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(augmented_data))\u001b[39m.\u001b[39mbatch(batch_size)\n\u001b[1;32m     93\u001b[0m train_dataset \u001b[39m=\u001b[39m train_dataset\u001b[39m.\u001b[39mprefetch(tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mAUTOTUNE)\n\u001b[1;32m     95\u001b[0m test_dataset \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices((test_image_paths, test_labels))\n",
      "File \u001b[0;32m~/miniconda3/envs/tf-nightly/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:1472\u001b[0m, in \u001b[0;36mDatasetV2.shuffle\u001b[0;34m(self, buffer_size, seed, reshuffle_each_iteration, name)\u001b[0m\n\u001b[1;32m   1383\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshuffle\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m   1384\u001b[0m             buffer_size,\n\u001b[1;32m   1385\u001b[0m             seed\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1386\u001b[0m             reshuffle_each_iteration\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1387\u001b[0m             name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1388\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Randomly shuffles the elements of this dataset.\u001b[39;00m\n\u001b[1;32m   1389\u001b[0m \n\u001b[1;32m   1390\u001b[0m \u001b[39m  This dataset fills a buffer with `buffer_size` elements, then randomly\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1470\u001b[0m \u001b[39m    A new `Dataset` with the transformation applied as described above.\u001b[39;00m\n\u001b[1;32m   1471\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1472\u001b[0m   \u001b[39mreturn\u001b[39;00m shuffle_op\u001b[39m.\u001b[39;49m_shuffle(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m   1473\u001b[0m       \u001b[39mself\u001b[39;49m, buffer_size, seed, reshuffle_each_iteration, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf-nightly/lib/python3.9/site-packages/tensorflow/python/data/ops/shuffle_op.py:31\u001b[0m, in \u001b[0;36m_shuffle\u001b[0;34m(input_dataset, buffer_size, seed, reshuffle_each_iteration, name)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_shuffle\u001b[39m(  \u001b[39m# pylint: disable=unused-private-name\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     input_dataset,\n\u001b[1;32m     27\u001b[0m     buffer_size,\n\u001b[1;32m     28\u001b[0m     seed\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     29\u001b[0m     reshuffle_each_iteration\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     30\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 31\u001b[0m   \u001b[39mreturn\u001b[39;00m _ShuffleDataset(\n\u001b[1;32m     32\u001b[0m       input_dataset, buffer_size, seed, reshuffle_each_iteration, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf-nightly/lib/python3.9/site-packages/tensorflow/python/data/ops/shuffle_op.py:56\u001b[0m, in \u001b[0;36m_ShuffleDataset.__init__\u001b[0;34m(self, input_dataset, buffer_size, seed, reshuffle_each_iteration, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name \u001b[39m=\u001b[39m name\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m (tf2\u001b[39m.\u001b[39menabled() \u001b[39mand\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     (context\u001b[39m.\u001b[39mexecuting_eagerly() \u001b[39mor\u001b[39;00m ops\u001b[39m.\u001b[39minside_function())):\n\u001b[0;32m---> 56\u001b[0m   variant_tensor \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39;49mshuffle_dataset_v3(\n\u001b[1;32m     57\u001b[0m       input_dataset\u001b[39m.\u001b[39;49m_variant_tensor,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     58\u001b[0m       buffer_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_buffer_size,\n\u001b[1;32m     59\u001b[0m       seed\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_seed,\n\u001b[1;32m     60\u001b[0m       seed2\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_seed2,\n\u001b[1;32m     61\u001b[0m       seed_generator\u001b[39m=\u001b[39;49mgen_dataset_ops\u001b[39m.\u001b[39;49mdummy_seed_generator(),\n\u001b[1;32m     62\u001b[0m       reshuffle_each_iteration\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reshuffle_each_iteration,\n\u001b[1;32m     63\u001b[0m       \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_common_args)\n\u001b[1;32m     64\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m   variant_tensor \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39mshuffle_dataset(\n\u001b[1;32m     66\u001b[0m       input_dataset\u001b[39m.\u001b[39m_variant_tensor,  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m     67\u001b[0m       buffer_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m       reshuffle_each_iteration\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reshuffle_each_iteration,\n\u001b[1;32m     71\u001b[0m       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_common_args)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf-nightly/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py:7117\u001b[0m, in \u001b[0;36mshuffle_dataset_v3\u001b[0;34m(input_dataset, buffer_size, seed, seed2, seed_generator, output_types, output_shapes, reshuffle_each_iteration, metadata, name)\u001b[0m\n\u001b[1;32m   7115\u001b[0m   \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   7116\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 7117\u001b[0m   _ops\u001b[39m.\u001b[39;49mraise_from_not_ok_status(e, name)\n\u001b[1;32m   7118\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_FallbackException:\n\u001b[1;32m   7119\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf-nightly/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:5875\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5873\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   5874\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m-> 5875\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__ShuffleDatasetV3_device_/job:localhost/replica:0/task:0/device:CPU:0}} buffer_size must be greater than zero or UNKNOWN_CARDINALITY [Op:ShuffleDatasetV3] name: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define the path to the 'fer2013' dataset directory\n",
    "train_dir = 'fer2013plus/fer2013/train'\n",
    "test_dir = 'fer2013plus/fer2013/test'\n",
    "\n",
    "# Create an ImageDataGenerator for data augmentation\n",
    "datagen_train = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,  # Random rotation\n",
    "    width_shift_range=0.1,  # Random horizontal shift\n",
    "    height_shift_range=0.1,  # Random vertical shift\n",
    "    shear_range=0.2,  # Shear transformation\n",
    "    zoom_range=0.2,  # Random zoom\n",
    "    horizontal_flip=True,  # Random horizontal flip\n",
    ")\n",
    "\n",
    "# Create an ImageDataGenerator for the test data (only rescale, no augmentation)\n",
    "datagen_test = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create empty lists to store image paths and labels\n",
    "train_image_paths = []\n",
    "train_labels = []\n",
    "test_image_paths = []\n",
    "test_labels = []\n",
    "\n",
    "# Function to load image paths and labels\n",
    "def load_image_paths_and_labels(directory, class_label):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            train_image_paths.append(filepath)\n",
    "            train_labels.append(class_label)\n",
    "\n",
    "# Load image paths and labels for each class in the train directory\n",
    "load_image_paths_and_labels(os.path.join(train_dir, 'happiness'), 0)\n",
    "load_image_paths_and_labels(os.path.join(train_dir, 'disgust'), 1)\n",
    "load_image_paths_and_labels(os.path.join(train_dir, 'surprise'), 2)\n",
    "load_image_paths_and_labels(os.path.join(train_dir, 'fear'), 3)\n",
    "load_image_paths_and_labels(os.path.join(train_dir, 'contempt'), 4)\n",
    "load_image_paths_and_labels(os.path.join(train_dir, 'neutral'), 5)\n",
    "load_image_paths_and_labels(os.path.join(train_dir, 'anger'), 6)\n",
    "load_image_paths_and_labels(os.path.join(train_dir, 'sadness'), 7)\n",
    "\n",
    "# Convert the lists to NumPy arrays\n",
    "train_image_paths = np.array(train_image_paths)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Perform data augmentation on the training data and create a balanced dataset\n",
    "num_augmented_samples_per_class = 1000\n",
    "augmented_data = []\n",
    "train=datagen_train.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(48, 48),\n",
    "        batch_size=1,\n",
    "        class_mode='sparse',\n",
    "        color_mode='grayscale',\n",
    "        shuffle=True,\n",
    "    )\n",
    "for _ in range(num_augmented_samples_per_class):\n",
    "    for batch in train:\n",
    "        if len(augmented_data) >= num_augmented_samples_per_class * len(np.unique(train_labels)):\n",
    "            break\n",
    "        augmented_data.append(batch[0][0])\n",
    "        train_labels = np.append(train_labels, batch[0][1])\n",
    "\n",
    "# Convert the augmented data and labels back to lists\n",
    "augmented_data = np.array(augmented_data)\n",
    "train_labels = train_labels.astype(int).tolist()\n",
    "\n",
    "# Load image paths and labels for the test directory\n",
    "load_image_paths_and_labels(os.path.join(test_dir, 'happiness'), 0)\n",
    "load_image_paths_and_labels(os.path.join(test_dir, 'disgust'), 1)\n",
    "load_image_paths_and_labels(os.path.join(test_dir, 'surprise'), 2)\n",
    "load_image_paths_and_labels(os.path.join(test_dir, 'fear'), 3)\n",
    "load_image_paths_and_labels(os.path.join(test_dir, 'contempt'), 4)\n",
    "load_image_paths_and_labels(os.path.join(test_dir, 'neutral'), 5)\n",
    "load_image_paths_and_labels(os.path.join(test_dir, 'anger'), 6)\n",
    "load_image_paths_and_labels(os.path.join(test_dir, 'sadness'), 7)\n",
    "\n",
    "# Convert the lists to NumPy arrays\n",
    "test_image_paths = np.array(test_image_paths)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# Create TensorFlow datasets for training and testing\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((augmented_data, train_labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(augmented_data)).batch(batch_size)\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_image_paths, test_labels))\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-nightly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
